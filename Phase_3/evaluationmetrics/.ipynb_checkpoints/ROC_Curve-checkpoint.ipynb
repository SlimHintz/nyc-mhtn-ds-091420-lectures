{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating a classification model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams[\"figure.figsize\"] = [10,5]\n",
    "\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-mhtn-ds-042219-lectures/master/Module_4/cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['youngin'] = titanic['Age']<=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S\n",
       "PassengerId                                                          \n",
       "1                 3  22.0      1      0   7.2500    False     1  0  1\n",
       "2                 1  38.0      1      0  71.2833    False     0  0  0\n",
       "3                 3  26.0      0      0   7.9250    False     0  0  1\n",
       "4                 1  35.0      1      0  53.1000    False     0  0  1\n",
       "5                 3  35.0      0      0   8.0500    False     1  0  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use x and y variables to split the training data into train and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict(zip(list(X.columns), list(logreg.coef_[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pclass': -0.8318946816510178,\n",
       " 'Age': -0.2726233827611389,\n",
       " 'SibSp': -0.41392398240940953,\n",
       " 'Parch': -0.1356348923966204,\n",
       " 'Fare': 0.0988848025772439,\n",
       " 'youngin': 0.483386338175624,\n",
       " 'male': -1.191642299745705,\n",
       " 'Q': -0.05395399240487809,\n",
       " 'S': -0.269550452369131}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What insights can we derive from these coefficients?\n",
    "\n",
    "On slack, send me a DM with one interpretations/insight you can take from these coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# make class predictions for the testing set\n",
    "preds = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Classification accuracy:** percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8251121076233184\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6188340807174888"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy (for binary classification problems coded as 0/1)\n",
    "max(y_test.mean(), 1 - y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.825112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.752941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.780488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.766467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "accuracy   0.825112\n",
       "recall     0.752941\n",
       "precision  0.780488\n",
       "F1         0.766467"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=[metrics.accuracy_score(y_test, preds), metrics.recall_score(y_test, preds),\n",
    "                   metrics.precision_score(y_test, preds), metrics.f1_score(y_test, preds)], \n",
    "             index=[\"accuracy\", \"recall\", \"precision\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.798206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.494118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.651163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "accuracy   0.798206\n",
       "recall     0.494118\n",
       "precision  0.954545\n",
       "F1         0.651163"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THRESHOLD = 0.75\n",
    "preds = np.where(logreg.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)\n",
    "\n",
    "pd.DataFrame(data=[metrics.accuracy_score(y_test, preds), metrics.recall_score(y_test, preds),\n",
    "                   metrics.precision_score(y_test, preds), metrics.f1_score(y_test, preds)], \n",
    "             index=[\"accuracy\", \"recall\", \"precision\", \"F1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall: A Tug of War\n",
    "To fully evaluate the effectiveness of a model, you must examine both precision and recall. Unfortunately, precision and recall are often in tension. That is, improving precision typically reduces recall and vice versa. Explore this notion by looking at the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/PrecisionVsRecallBase.svg' width=900/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to map a logistic regression value to a binary category, you must define a classification threshold (also called the decision threshold). A value above that threshold indicates \"spam\"; a value below indicates \"not spam.\" It is tempting to assume that the classification threshold should always be 0.5, but thresholds are problem-dependent, and are therefore values that you must tune.\n",
    "\n",
    "\n",
    "\"Tuning\" a threshold for logistic regression is different from tuning hyperparameters such as learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those to the right of the classification threshold are classified as \"spam\", while those to the left are classified as \"not spam.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ROC Curves and Area Under the Curve (AUC)\n",
    "\n",
    "**Question:** Wouldn't it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?\n",
    "\n",
    "**Answer:** Plot the ROC curve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "\n",
    "- True Positive Rate\n",
    "- False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive Rate (TPR)** is a synonym for recall and is therefore defined as follows:\n",
    "\n",
    "$$TPR = \\frac{TP} {TP + FN}$$\n",
    "\n",
    "**False Positive Rate (FPR)** is defined as follows:\n",
    "\n",
    "\n",
    "$$FPR = \\frac{FP} {FP + TN}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](images/perfectImbalance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![alt text](images/perfectbalanceAUC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](images/overlapimbalance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![alt text](images/imbalancedROC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC: Area Under the ROC Curve\n",
    "**AUC** stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](images/perfectimbalanced.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. For example, given the following examples, which are arranged from left to right in ascending order of logistic regression predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the probability predicitons\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for Titanic classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.00724638, 0.00724638,\n",
       "       0.01449275, 0.01449275, 0.02173913, 0.02173913, 0.02898551,\n",
       "       0.02898551, 0.04347826, 0.04347826, 0.05072464, 0.05072464,\n",
       "       0.05797101, 0.05797101, 0.06521739, 0.06521739, 0.07971014,\n",
       "       0.07971014, 0.08695652, 0.08695652, 0.13043478, 0.13043478,\n",
       "       0.13768116, 0.13768116, 0.14492754, 0.14492754, 0.15217391,\n",
       "       0.15217391, 0.15942029, 0.15942029, 0.16666667, 0.17391304,\n",
       "       0.17391304, 0.19565217, 0.19565217, 0.20289855, 0.20289855,\n",
       "       0.2826087 , 0.29710145, 0.29710145, 0.34782609, 0.34782609,\n",
       "       0.37681159, 0.37681159, 0.44202899, 0.44202899, 0.45652174,\n",
       "       0.51449275, 0.53623188, 0.60869565, 0.62318841, 0.65217391,\n",
       "       0.65217391, 0.67391304, 0.70289855, 0.73913043, 0.73913043,\n",
       "       0.77536232, 0.7826087 , 0.7826087 , 0.8115942 , 0.8115942 ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.016184</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100034</th>\n",
       "      <td>0.811594</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100142</th>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.988235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.103838</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.988235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.103843</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.976471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.103902</th>\n",
       "      <td>0.775362</td>\n",
       "      <td>0.976471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.103916</th>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.976471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.105403</th>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.964706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.107748</th>\n",
       "      <td>0.702899</td>\n",
       "      <td>0.964706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.109670</th>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.964706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.110968</th>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.964706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.111670</th>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.952941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.113712</th>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.952941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.113751</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.952941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.149266</th>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.952941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.151728</th>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.174829</th>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.180769</th>\n",
       "      <td>0.442029</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.180771</th>\n",
       "      <td>0.442029</td>\n",
       "      <td>0.929412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.218958</th>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.929412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.220390</th>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.917647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.225989</th>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.917647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.228976</th>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.905882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.274575</th>\n",
       "      <td>0.297101</td>\n",
       "      <td>0.905882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.283142</th>\n",
       "      <td>0.297101</td>\n",
       "      <td>0.894118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.284279</th>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.894118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.348850</th>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.894118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.349520</th>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.355546</th>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.358518</th>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.870588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.386308</th>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.870588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.398155</th>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.858824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.414060</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.858824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.416141</th>\n",
       "      <td>0.159420</td>\n",
       "      <td>0.847059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.419161</th>\n",
       "      <td>0.159420</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.425587</th>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.429944</th>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.811765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.459018</th>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.811765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.473889</th>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.788235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.477399</th>\n",
       "      <td>0.137681</td>\n",
       "      <td>0.788235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.484957</th>\n",
       "      <td>0.137681</td>\n",
       "      <td>0.776471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491204</th>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.776471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.521516</th>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.741176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.575754</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.741176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.585768</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.591237</th>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.596664</th>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.670588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.597523</th>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.670588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.613849</th>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.658824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.615984</th>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.658824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.681448</th>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.611765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.681513</th>\n",
       "      <td>0.050725</td>\n",
       "      <td>0.611765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.681722</th>\n",
       "      <td>0.050725</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.684644</th>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.693012</th>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.564706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.699412</th>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.564706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.712298</th>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.552941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.715718</th>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.552941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.716064</th>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.541176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.720977</th>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.541176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.770574</th>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.447059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.779678</th>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.447059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.929339</th>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.129412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.930549</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.982121</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.982121</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               FPR       TPR\n",
       "0.016184  1.000000  1.000000\n",
       "0.100034  0.811594  1.000000\n",
       "0.100142  0.811594  0.988235\n",
       "0.103838  0.782609  0.988235\n",
       "0.103843  0.782609  0.976471\n",
       "0.103902  0.775362  0.976471\n",
       "0.103916  0.739130  0.976471\n",
       "0.105403  0.739130  0.964706\n",
       "0.107748  0.702899  0.964706\n",
       "0.109670  0.673913  0.964706\n",
       "0.110968  0.652174  0.964706\n",
       "0.111670  0.652174  0.952941\n",
       "0.113712  0.623188  0.952941\n",
       "0.113751  0.608696  0.952941\n",
       "0.149266  0.536232  0.952941\n",
       "0.151728  0.514493  0.941176\n",
       "0.174829  0.456522  0.941176\n",
       "0.180769  0.442029  0.941176\n",
       "0.180771  0.442029  0.929412\n",
       "0.218958  0.376812  0.929412\n",
       "0.220390  0.376812  0.917647\n",
       "0.225989  0.347826  0.917647\n",
       "0.228976  0.347826  0.905882\n",
       "0.274575  0.297101  0.905882\n",
       "0.283142  0.297101  0.894118\n",
       "0.284279  0.282609  0.894118\n",
       "0.348850  0.202899  0.894118\n",
       "0.349520  0.202899  0.882353\n",
       "0.355546  0.195652  0.882353\n",
       "0.358518  0.195652  0.870588\n",
       "0.386308  0.173913  0.870588\n",
       "0.398155  0.173913  0.858824\n",
       "0.414060  0.166667  0.858824\n",
       "0.416141  0.159420  0.847059\n",
       "0.419161  0.159420  0.823529\n",
       "0.425587  0.152174  0.823529\n",
       "0.429944  0.152174  0.811765\n",
       "0.459018  0.144928  0.811765\n",
       "0.473889  0.144928  0.788235\n",
       "0.477399  0.137681  0.788235\n",
       "0.484957  0.137681  0.776471\n",
       "0.491204  0.130435  0.776471\n",
       "0.521516  0.130435  0.741176\n",
       "0.575754  0.086957  0.741176\n",
       "0.585768  0.086957  0.705882\n",
       "0.591237  0.079710  0.705882\n",
       "0.596664  0.079710  0.670588\n",
       "0.597523  0.065217  0.670588\n",
       "0.613849  0.065217  0.658824\n",
       "0.615984  0.057971  0.658824\n",
       "0.681448  0.057971  0.611765\n",
       "0.681513  0.050725  0.611765\n",
       "0.681722  0.050725  0.588235\n",
       "0.684644  0.043478  0.588235\n",
       "0.693012  0.043478  0.564706\n",
       "0.699412  0.028986  0.564706\n",
       "0.712298  0.028986  0.552941\n",
       "0.715718  0.021739  0.552941\n",
       "0.716064  0.021739  0.541176\n",
       "0.720977  0.014493  0.541176\n",
       "0.770574  0.014493  0.447059\n",
       "0.779678  0.007246  0.447059\n",
       "0.929339  0.007246  0.129412\n",
       "0.930549  0.000000  0.129412\n",
       "0.982121  0.000000  0.011765\n",
       "1.982121  0.000000  0.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([fpr,tpr], index=['FPR','TPR'], columns = thresholds).T.sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ROC curve can help you to **choose a threshold** that balances sensitivity and specificity in a way that makes sense for your particular context\n",
    "- You can't actually **see the thresholds** used to generate the curve on the ROC curve itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a threshold and prints sensitivity and specificity\n",
    "def evaluate_threshold(threshold):\n",
    "    print('Sensitivity:', tpr[thresholds > threshold][-1])\n",
    "    print('Specificity:', 1 - fpr[thresholds > threshold][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.7411764705882353\n",
      "Specificity: 0.8695652173913043\n"
     ]
    }
   ],
   "source": [
    "evaluate_threshold(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.8941176470588236\n",
      "Specificity: 0.7971014492753623\n"
     ]
    }
   ],
   "source": [
    "evaluate_threshold(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "AUC is the **percentage** of the ROC plot that is **underneath the curve**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9017902813299232\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
    "print(metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- AUC is useful as a **single number summary** of classifier performance.\n",
    "- If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a **higher predicted probability** to the positive observation.\n",
    "- AUC is useful even when there is **high class imbalance** (unlike classification accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8565092097445038"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate cross-validated AUC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will regularization get us a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new LogisticRegression object using regrularization\n",
    "logR = LogisticRegression(penalty='l2', C=0.2 )\n",
    "\n",
    "#fit that new model to the training data\n",
    "logR.fit(X_train, y_train)\n",
    "\n",
    "#use that new model to create predictions on the test data\n",
    "predictions = logR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99,\n",
       "       1.  ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.linspace(0.01, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.09, 0.8352514919011083),\n",
       " (0.09999999999999999, 0.828005115089514),\n",
       " (0.11, 0.828005115089514),\n",
       " (0.060000000000000005, 0.8271099744245525),\n",
       " (0.08, 0.8257459505541347),\n",
       " (0.12, 0.8243819266837169),\n",
       " (0.13, 0.8243819266837169),\n",
       " (0.14, 0.8243819266837169),\n",
       " (0.15000000000000002, 0.8243819266837169),\n",
       " (0.16, 0.8243819266837169),\n",
       " (0.06999999999999999, 0.8234867860187554),\n",
       " (0.05, 0.8221227621483376),\n",
       " (0.17, 0.8207587382779198),\n",
       " (0.01, 0.8184995737425406),\n",
       " (0.02, 0.8184995737425406),\n",
       " (0.03, 0.8184995737425406),\n",
       " (0.04, 0.8184995737425406),\n",
       " (0.18000000000000002, 0.8171355498721228),\n",
       " (0.19, 0.8171355498721228),\n",
       " (0.2, 0.8171355498721228),\n",
       " (0.21000000000000002, 0.8171355498721228),\n",
       " (0.22, 0.8171355498721228),\n",
       " (0.23, 0.8171355498721228),\n",
       " (0.24000000000000002, 0.8171355498721228),\n",
       " (0.25, 0.8171355498721228),\n",
       " (0.26, 0.8171355498721228),\n",
       " (0.27, 0.8171355498721228),\n",
       " (0.28, 0.8171355498721228),\n",
       " (0.29000000000000004, 0.8171355498721228),\n",
       " (0.3, 0.8171355498721228),\n",
       " (0.31, 0.8171355498721228),\n",
       " (0.32, 0.8171355498721228),\n",
       " (0.33, 0.8171355498721228),\n",
       " (0.34, 0.8171355498721228),\n",
       " (0.35000000000000003, 0.8171355498721228),\n",
       " (0.36000000000000004, 0.8171355498721228),\n",
       " (0.37, 0.8171355498721228),\n",
       " (0.38, 0.8171355498721228),\n",
       " (0.39, 0.8171355498721228),\n",
       " (0.4, 0.8112531969309463),\n",
       " (0.41000000000000003, 0.8112531969309463),\n",
       " (0.42000000000000004, 0.8112531969309463),\n",
       " (0.43, 0.8112531969309463),\n",
       " (0.44, 0.8112531969309463),\n",
       " (0.45, 0.8112531969309463),\n",
       " (0.46, 0.8112531969309463),\n",
       " (0.47000000000000003, 0.8112531969309463),\n",
       " (0.48000000000000004, 0.8112531969309463),\n",
       " (0.49, 0.8112531969309463),\n",
       " (0.5, 0.8112531969309463),\n",
       " (0.51, 0.8112531969309463),\n",
       " (0.52, 0.8112531969309463),\n",
       " (0.53, 0.8112531969309463),\n",
       " (0.54, 0.8112531969309463),\n",
       " (0.55, 0.8112531969309463),\n",
       " (0.56, 0.8112531969309463),\n",
       " (0.5700000000000001, 0.8112531969309463),\n",
       " (0.5800000000000001, 0.8112531969309463),\n",
       " (0.59, 0.8112531969309463),\n",
       " (0.6, 0.8112531969309463),\n",
       " (0.61, 0.8112531969309463),\n",
       " (0.62, 0.8112531969309463),\n",
       " (0.63, 0.8112531969309463),\n",
       " (0.64, 0.8112531969309463),\n",
       " (0.65, 0.8112531969309463),\n",
       " (0.66, 0.8112531969309463),\n",
       " (0.67, 0.8112531969309463),\n",
       " (0.68, 0.8112531969309463),\n",
       " (0.6900000000000001, 0.8112531969309463),\n",
       " (0.7000000000000001, 0.8112531969309463),\n",
       " (0.7100000000000001, 0.8112531969309463),\n",
       " (0.72, 0.8112531969309463),\n",
       " (0.73, 0.8112531969309463),\n",
       " (0.74, 0.8112531969309463),\n",
       " (0.75, 0.8112531969309463),\n",
       " (0.76, 0.8112531969309463),\n",
       " (0.77, 0.8112531969309463),\n",
       " (0.78, 0.8112531969309463),\n",
       " (0.79, 0.8112531969309463),\n",
       " (0.8, 0.8112531969309463),\n",
       " (0.81, 0.8112531969309463),\n",
       " (0.8200000000000001, 0.8112531969309463),\n",
       " (0.8300000000000001, 0.8112531969309463),\n",
       " (0.8400000000000001, 0.8112531969309463),\n",
       " (0.85, 0.8112531969309463),\n",
       " (0.86, 0.8112531969309463),\n",
       " (0.87, 0.8112531969309463),\n",
       " (0.88, 0.8112531969309463),\n",
       " (0.89, 0.8112531969309463),\n",
       " (0.9, 0.8112531969309463),\n",
       " (0.91, 0.8112531969309463),\n",
       " (0.92, 0.8112531969309463),\n",
       " (0.93, 0.8112531969309463),\n",
       " (0.9400000000000001, 0.8112531969309463),\n",
       " (0.9500000000000001, 0.8112531969309463),\n",
       " (0.9600000000000001, 0.8112531969309463),\n",
       " (0.97, 0.8112531969309463),\n",
       " (0.98, 0.8112531969309463),\n",
       " (0.99, 0.8112531969309463),\n",
       " (1.0, 0.8112531969309463)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_values = list(np.linspace(0.01, 1, 100))\n",
    "models = [LogisticRegression(penalty='l1', C=value, solver='saga') for value in c_values]\n",
    "\n",
    "fit = [model.fit(X_train, y_train) for model in models]\n",
    "\n",
    "predictions = [model.predict(X_test) for model in fit]\n",
    "\n",
    "auc = [metrics.roc_auc_score(y_test, prediction) for prediction in predictions]\n",
    "\n",
    "all_the_things = sorted(list(zip([x for x in c_values], auc)), key=lambda x: x[1], reverse=True)\n",
    "all_the_things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C == alpha\n",
    "\n",
    "Our regularization strength is inversely proportional to the size of c. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if the model has a better AUC score than the unregularized model\n",
    "\n",
    "print(metrics.roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example.\n",
    "\n",
    "AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n",
    "\n",
    "AUC is desirable for the following two reasons:\n",
    "\n",
    "- AUC is **scale-invariant**. It measures how well predictions are ranked, rather than their absolute values.\n",
    "- AUC is **classification-threshold-invariant**. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Two Caveats\n",
    "\n",
    "Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.\n",
    "\n",
    "Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Confusion matrix advantages:**\n",
    "\n",
    "- Allows you to calculate a **variety of metrics**\n",
    "- Useful for **multi-class problems** (more than two response classes)\n",
    "\n",
    "**ROC/AUC advantages:**\n",
    "\n",
    "- Does not require you to **set a classification threshold**\n",
    "- Still useful when there is **high class imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion Matrix Resources\n",
    "\n",
    "- Blog post: [Simple guide to confusion matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) by me\n",
    "- Videos: [Intuitive sensitivity and specificity](https://www.youtube.com/watch?v=U4_3fditnWg) (9 minutes) and [The tradeoff between sensitivity and specificity](https://www.youtube.com/watch?v=vtYDyGGeQyo) (13 minutes) by Rahul Patwari\n",
    "- Notebook: [How to calculate \"expected value\"](https://github.com/podopie/DAT18NYC/blob/master/classes/13-expected_value_cost_benefit_analysis.ipynb) from a confusion matrix by treating it as a cost-benefit matrix (by Ed Podojil)\n",
    "- Graphic: How [classification threshold](https://media.amazonwebservices.com/blog/2015/ml_adjust_model_1.png) affects different evaluation metrics (from a [blog post](https://aws.amazon.com/blogs/aws/amazon-machine-learning-make-data-driven-decisions-at-scale/) about Amazon Machine Learning)\n",
    "\n",
    "\n",
    "## ROC and AUC Resources\n",
    "\n",
    "- Video: [ROC Curves and Area Under the Curve](https://www.youtube.com/watch?v=OAl6eAyP-yo) (14 minutes) by Kevin Markham, including [transcript and screenshots](http://www.dataschool.io/roc-curves-and-auc-explained/) and a [visualization](http://www.navan.name/roc/)\n",
    "- Video: [ROC Curves](https://www.youtube.com/watch?v=21Igj5Pr6u4) (12 minutes) by Rahul Patwari\n",
    "- Paper: [An introduction to ROC analysis](http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf) by Tom Fawcett\n",
    "- Usage examples: [Comparing different feature sets](http://research.microsoft.com/pubs/205472/aisec10-leontjeva.pdf) for detecting fraudulent Skype users, and [comparing different classifiers](http://www.cse.ust.hk/nevinZhangGroup/readings/yi/Bradley_PR97.pdf) on a number of popular datasets\n",
    "\n",
    "\n",
    "## Other Resources\n",
    "\n",
    "- scikit-learn documentation: [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- Guide: [Comparing model evaluation procedures and metrics](https://github.com/justmarkham/DAT8/blob/master/other/model_evaluation_comparison.md) by me\n",
    "- Video: [Counterfactual evaluation of machine learning models](https://www.youtube.com/watch?v=QWCSxAKR-h0) (45 minutes) about how Stripe evaluates its fraud detection model, including [slides](http://www.slideshare.net/MichaelManapat/counterfactual-evaluation-of-machine-learning-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
